{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport torch.nn.functional as F\nimport torchvision.models as models\nimport numpy as np\nimport torchvision\nfrom torchvision import datasets, models, transforms\nimport matplotlib.pyplot as plt\nimport time\nimport os\nimport copy\nimport argparse\nfrom torch.utils.data import random_split\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import *\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nfrom shutil import copyfile, make_archive\nfrom IPython.display import FileLink","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = '/kaggle/input/isic-2017-preprocessed-augmented/content/Linear_Exact_Aug'\n\n\nmean = np.array([0.485, 0.456, 0.406])\nstd = np.array([0.229, 0.224, 0.225])\n\ndata_transforms = {\n    'Train': transforms.Compose([\n        transforms.Resize((224,224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std)\n    ]),\n    'Test': transforms.Compose([\n        transforms.Resize((224,224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std)\n    ]),\n}\n\nimage_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n                                          data_transforms[x])\n                  for x in ['Train', 'Test']}\nimage_datasets['Valid']=datasets.ImageFolder(os.path.join(data_dir,'Valid'),data_transforms['Train'])\ntrain_size=len(image_datasets['Train']) \nvalidation_size=len(image_datasets['Valid'])\n\ndataloaders={x: torch.utils.data.DataLoader(image_datasets[x],batch_size=8,shuffle=True,num_workers=10)\n             for x in ['Train','Valid']}\n\ndataset_sizes = {'train':train_size,'val':validation_size,'test':len(image_datasets['Test'])}\nclass_names = image_datasets['Train'].classes\nnum_classes = len(class_names)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SelfAttention(nn.Module):\n    \"\"\" Self attention Layer\"\"\"\n    def __init__(self, in_dim):\n        super(SelfAttention, self).__init__()\n        self.chanel_in = in_dim\n        #print(in_dim)\n        self.query_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\n        self.key_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\n        self.value_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=1)\n        self.gamma = nn.Parameter(torch.zeros(1))\n        self.softmax = nn.Softmax(dim=-1)\n        \n    def forward(self, x):\n        m_batchsize, C, width, height = x.size()\n        proj_query = self.query_conv(x).view(m_batchsize, -1, width*height).permute(0, 2, 1) # B X (N) X C\n        proj_key = self.key_conv(x).view(m_batchsize, -1, width*height) # B X C x (*W*H)\n        energy = torch.bmm(proj_query, proj_key) # transpose check\n        attention = self.softmax(energy) # BX (N) X (N) \n        proj_value = self.value_conv(x).view(m_batchsize, -1, width*height) # B X C X N\n\n        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n        out = out.view(m_batchsize, C, width, height)\n        out = self.gamma * out + x\n        return out, attention\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class VGG11WithSelfAttention(nn.Module):\n    def __init__(self, num_classes):\n        super(VGG11WithSelfAttention, self).__init__()\n        # Load pre-trained VGG11 model\n        self.vgg11 = models.vgg11_bn(pretrained=True)\n        \n        # Find the last convolutional layer\n        last_conv_layer = None\n        for layer in reversed(self.vgg11.features):\n            if isinstance(layer, nn.Conv2d):\n                last_conv_layer = layer\n                break\n        \n        # Get the number of output channels of the last convolutional layer\n        num_ftrs = last_conv_layer.out_channels\n        \n        # Modify the classifier to include Self Attention and final classification layer\n        self.self_attention = SelfAttention(num_ftrs)\n        self.classifier = nn.Linear(num_ftrs, num_classes)\n        \n    def forward(self, x):\n        features = self.vgg11.features(x)\n        # Pass features through self-attention layer\n        #print(features.size())\n        out, _ = self.self_attention(features)\n        # Apply ReLU activation\n        out = F.relu(out, inplace=True)\n        # Apply adaptive average pooling\n        out = F.adaptive_avg_pool2d(out, (1, 1))\n        # Flatten the tensor\n        out = torch.flatten(out, 1)\n        # Pass through the classifier\n        out = self.classifier(out)\n        return out","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Client:\n    def __init__(self, client_id, data_dir):\n        self.id = client_id  # Unique id for each client\n        self.data_dir = data_dir\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        self.test_dir = \"/kaggle/input/ham10000-data/HAM10000_DATA/test_dir\"\n        self.num_classes, self.dataloaders, self.dataset_sizes = self.initData()\n        self.model = self.build_model()\n        self.model_name = f'Client_{self.id}'\n\n    def initData(self):\n        '''\n            Initialize the train and validation dataloaders\n            Split the train directory into train and val dataloaders.\n            80% images for train 20% for validation\n        '''\n        mean = np.array([0.485, 0.456, 0.406])\n        std = np.array([0.229, 0.224, 0.225])\n\n        data_transforms = {\n            'train_dir': transforms.Compose([\n                transforms.Resize((224, 224)),\n                transforms.ToTensor(),\n                transforms.Normalize(mean, std)\n            ]),\n            'test_dir': transforms.Compose([\n                transforms.Resize((224, 224)),\n                transforms.ToTensor(),\n                transforms.Normalize(mean, std)\n            ]),\n        }\n\n        image_datasets = {'train_dir': datasets.ImageFolder(self.data_dir,data_transforms['train_dir'])\n                          }\n\n        train_size = int(0.8 * len(image_datasets['train_dir']))  # 80% of images in train_dir will form training set\n        validation_size = len(image_datasets['train_dir']) - train_size  # Remaining 20% of images in train_dir will form validation set\n        train_dataset, validation_dataset = random_split(image_datasets['train_dir'], [train_size, validation_size])\n        dataloaders = {}\n        dataloaders['train'] = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=10)\n        dataloaders['val'] = torch.utils.data.DataLoader(validation_dataset, batch_size=8, shuffle=True, num_workers=10)\n\n        dataset_sizes = {'train': len(train_dataset), 'val': len(validation_dataset)}\n        class_names = image_datasets['train_dir'].classes\n        num_classes = len(class_names)\n        return num_classes, dataloaders, dataset_sizes\n\n    def build_model(self):\n        # Replace VGG11WithSelfAttention with your actual model class\n        model = VGG11WithSelfAttention(self.num_classes).to(self.device)\n        return model\n    \n    def calculate_confidences(self, model_paths, validation_loaders):\n        confidences = []        \n        models=[]\n        for path in model_paths:\n            model=self.build_model()\n            state_dict=torch.load(path)\n            model.load_state_dict(state_dict)\n            #model.to(self.device)\n            model.eval()\n            models.append(model)\n        loader = validation_loaders\n        for model in models: #zip(models, validation_loaders):            \n            total_confidence = 0\n            with torch.no_grad():\n                for inputs, targets in loader:\n                    inputs = inputs.to(self.device)\n                    targets = targets.to(self.device)\n                    outputs = model(inputs)\n                    probabilities = torch.nn.functional.softmax(outputs, dim=1)\n                    total_confidence += probabilities.max(dim=1)[0].mean().item()\n            average_confidence = total_confidence / len(loader)\n            confidences.append(average_confidence)\n\n        total_confidence = sum(confidences)\n        normalized_confidences = [conf / total_confidence for conf in confidences]\n        return normalized_confidences\n    \n    def confidence_weighted_aggregation(self, model_paths, confidences):\n        aggregated_weights = {}        \n        models=[]\n        for path in model_paths:\n            model=self.build_model()\n            state_dict=torch.load(path)\n            model.load_state_dict(state_dict)\n            #model.to(self.device)\n            model.eval()\n            models.append(model)\n        \n        confidences_tensor = torch.tensor(confidences, device=self.device)\n        aggregated_weights = {}\n        if len(models) == 1:\n            # If there's only one model, directly use its weights\n            aggregated_weights = models[0].state_dict()\n        else:\n            for key in models[0].state_dict().keys():\n                layer_weights = torch.stack([model.state_dict()[key] for model in models], dim=0)\n                # Adjust the view of confidences_tensor to match the dimensions of layer_weights\n                if layer_weights.dim() == 1:  # Bias terms\n                    weighted_avg = torch.sum(layer_weights * confidences_tensor, dim=0) / torch.sum(confidences_tensor)\n                elif layer_weights.dim() == 2:  # Fully connected layers\n                    weighted_avg = torch.sum(layer_weights * confidences_tensor.view(-1, 1), dim=0) / torch.sum(confidences_tensor)\n                elif layer_weights.dim() == 4:  # Convolutional layers\n                    weighted_avg = torch.sum(layer_weights * confidences_tensor.view(-1, 1, 1, 1), dim=0) / torch.sum(confidences_tensor)\n                else:  # Catch-all case for other dimensions\n                    expanded_confidences = confidences_tensor.view([-1] + [1] * (layer_weights.dim() - 1))\n                    weighted_avg = torch.sum(layer_weights * expanded_confidences, dim=0) / torch.sum(confidences_tensor)\n                aggregated_weights[key] = weighted_avg                \n\n        return aggregated_weights\n    \n    def set_weights(self, wts):\n        self.model.load_state_dict(torch.load(wts))  # Load weights to client model.\n\n    def plot_loss(self, train_loss, val_loss):\n        epochs = range(1, len(train_loss) + 1)\n        plt.plot(epochs, train_loss, 'b', label='Training loss')\n        plt.plot(epochs, val_loss, 'r', label='Validation loss')\n        plt.title('Training and Validation Loss')\n        plt.xlabel('Epochs')\n        plt.ylabel('Loss')\n        plt.legend()\n        plt.show()\n\n    def plot_acc(self, train_acc, val_acc):\n        epochs = range(1, len(train_acc) + 1)\n        plt.plot(epochs, train_acc, 'b', label='Training accuracy')\n        plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n        plt.title('Training and Validation Accuracy')\n        plt.xlabel('Epochs')\n        plt.ylabel('Accuracy')\n        plt.legend()\n        plt.show()\n\n    def aggregate_weights(self, prev_models):\n        prev_models = [torch.load(path) for path in prev_models]\n        avg_state_dict = copy.deepcopy(prev_models[0])\n        for key in avg_state_dict.keys():\n            for i in range(1, len(prev_models)):\n                avg_state_dict[key] += prev_models[i][key]\n            avg_state_dict[key] = torch.div(avg_state_dict[key], len(prev_models))\n        self.model.load_state_dict(avg_state_dict)\n\n    def train_model(self, epochs=1, prev_models=None):\n        if prev_models:\n            #self.aggregate_weights(prev_models)\n            confidences=self.calculate_confidences(prev_models,self.dataloaders['val'])            \n            new_wts=self.confidence_weighted_aggregation(prev_models,confidences)\n             # Debugging step: print shapes of new_wts and current model parameters\n#             for key in new_wts.keys():\n#                 print(f\"Layer: {key}, Aggregated weight shape: {new_wts[key].shape}, Model weight shape: {self.model.state_dict()[key].shape}\")\n            \n            self.model.load_state_dict(new_wts)\n        criterion = nn.CrossEntropyLoss()\n        optimizer = optim.SGD(self.model.parameters(), lr=0.0001, momentum=0.99)\n        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n        val_loss_gph = []\n        train_loss_gph = []\n        val_acc_gph = []\n        train_acc_gph = []\n        since = time.time()\n\n        best_model_wts = copy.deepcopy(self.model.state_dict())\n        best_loss = float(\"inf\")\n        best_acc = 0\n        for epoch in range(epochs):\n            print('Epoch {}/{}'.format(epoch + 1, epochs))\n            print('-' * 10)\n\n            # Each epoch has a training and validation phase\n            for phase in ['train', 'val']:\n                if phase == 'train':\n                    self.model.train()  # Set model to training mode\n                else:\n                    self.model.eval()  # Set model to evaluate mode\n\n                running_loss = 0.0\n                running_corrects = 0\n                p = phase\n\n                # Wrap data loader with tqdm for progress bar\n                data_loader = tqdm(self.dataloaders[p], desc=f'{phase} Epoch {epoch + 1}/{epochs}')\n\n                for inputs, labels in data_loader:\n                    inputs = inputs.to(self.device)\n                    labels = labels.to(self.device)\n\n                    # forward\n                    # track history if only in train\n                    with torch.set_grad_enabled(phase == 'train'):\n                        outputs = self.model(inputs)\n                        _, preds = torch.max(outputs, 1)\n                        loss = criterion(outputs, labels)\n\n                        # backward + optimize only if in training phase\n                        if phase == 'train':\n                            optimizer.zero_grad()\n                            loss.backward()\n                            optimizer.step()\n\n                    # statistics\n                    running_loss += loss.item() * inputs.size(0)\n                    running_corrects += torch.sum(preds == labels.data)\n\n                    # Update tqdm progress bar description\n                    data_loader.set_postfix({'loss': loss.item()})\n\n                if phase == 'train':\n                    scheduler.step()\n                    epoch_loss = running_loss / self.dataset_sizes[p]\n                    epoch_acc = running_corrects.double() / self.dataset_sizes[p]\n                    train_loss_gph.append(epoch_loss)\n                    train_acc_gph.append(epoch_acc)\n                else:\n                    epoch_loss = running_loss / self.dataset_sizes[p]\n                    epoch_acc = running_corrects.double() / self.dataset_sizes[p]\n                    val_loss_gph.append(epoch_loss)\n                    val_acc_gph.append(epoch_acc)\n\n                print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n\n                if phase == 'val' and epoch_acc > best_acc:\n                    best_acc = epoch_acc\n                    best_model_wts = copy.deepcopy(self.model.state_dict())\n                    torch.save(self.model.state_dict(), \"/kaggle/working/\" + \"/\" + self.model_name + \".pth\")\n                    print('==>Model Saved')\n\n            print()\n\n        time_elapsed = time.time() - since\n        print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n        print('Best val acc: {:4f}'.format(best_acc))\n        for i in range(len(train_acc_gph)):\n            train_acc_gph[i]=train_acc_gph[i].cpu() #Convert to numpy tensors to enable plotting\n\n        for i in range(len(val_acc_gph)):\n            val_acc_gph[i]=val_acc_gph[i].cpu()\n        self.plot_loss(train_loss_gph, val_loss_gph)\n        self.plot_acc(train_acc_gph, val_acc_gph)\n        self.model.load_state_dict(best_model_wts)\n\n    def metrics(self, labels, predictions, classes, y_true, y_prob):\n        print(\"Classification Report:\")\n        print(classification_report(labels, predictions, target_names=classes, digits=4))\n        matrix = confusion_matrix(labels, predictions)\n        print(\"Confusion matrix:\")\n        print(matrix)\n        print(\"Heat map:\")\n        fig, ax = plt.subplots(figsize=(8, 6))\n        sns.heatmap(matrix, annot=True, xticklabels=classes, yticklabels=classes, cmap=plt.cm.Blues, fmt='.2f')\n        plt.ylabel('Actual Classes')\n        plt.xlabel('Predicted Classes')\n        plt.show(block=False)\n\n        print(\"Precision: \" + str(precision_score(labels, predictions, average='weighted')))\n        print(\"Recall: \" + str(recall_score(labels, predictions, average='weighted')))\n        print(\"Accuracy: \" + str(accuracy_score(labels, predictions)))\n        f1 = f1_score(labels, predictions, average='weighted')\n        print(\"F1 Score: \" + str(f1))\n\n        print(\"Precision: \" + str(precision_score(labels, predictions, average='macro')))\n        print(\"Recall: \" + str(recall_score(labels, predictions, average='macro')))\n        print(\"Accuracy: \" + str(accuracy_score(labels, predictions)))\n        f1 = f1_score(labels, predictions, average='macro')\n        print(\"F1 Score: \" + str(f1))\n\n        print(\"Precision: \" + str(precision_score(labels, predictions, average='micro')))\n        print(\"Recall: \" + str(recall_score(labels, predictions, average='micro')))\n        print(\"Accuracy: \" + str(accuracy_score(labels, predictions)))\n        f1 = f1_score(labels, predictions, average='micro')\n        print(\"F1 Score: \" + str(f1))\n        fpr = dict()\n        tpr = dict()\n        roc_auc = dict()\n\n        for i in range(len(classes)):\n            fpr[i], tpr[i], _ = roc_curve(y_true == i, y_prob[:, i])\n            roc_auc[i] = auc(fpr[i], tpr[i])\n\n        # Plot the ROC curves\n        plt.figure()\n        for i in range(len(classes)):\n            plt.plot(fpr[i], tpr[i], label=f'Class {i} (AUC = {roc_auc[i]:.2f})')\n\n        plt.plot([0, 1], [0, 1], 'k--')\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title('ROC Curve')\n        plt.legend(loc='lower right')\n        plt.show()\n        print(\"weighted Roc score: \" + str(roc_auc_score(y_true, y_prob, multi_class='ovr', average='weighted')))\n        print(\"macro Roc score: \" + str(roc_auc_score(y_true, y_prob, multi_class='ovr', average='macro')))\n        print(\"micro Roc score: \" + str(roc_auc_score(y_true, y_prob, multi_class='ovr', average='micro')))\n\n        print(\"\\nClasswise Accuracy :{}\".format(matrix.diagonal() / matrix.sum(axis=1)))\n        print(\"\\nBalanced Accuracy Score: \", balanced_accuracy_score(labels, predictions))\n\n    def get_predictions(self):\n        testloader = torch.utils.data.DataLoader(image_datasets['Test'], batch_size=4, shuffle=False, num_workers=4)\n        self.model.eval()\n        predictions = []\n        true_labels = []\n        criterion = nn.CrossEntropyLoss()\n        test_loss = 0.0\n        num_samples = 0\n\n        y_true = []  # For ROC curve\n        y_prob = []  # For ROC curve\n        with torch.no_grad():\n            for inputs, labels in testloader:\n                inputs = inputs.to(self.device)\n                labels = labels.to(self.device)\n                outputs = self.model(inputs)\n                _, predicted = torch.max(outputs, 1)\n                prob = torch.nn.functional.softmax(outputs, dim=1)\n                # Compute the loss\n                loss = criterion(outputs, labels)\n\n                # Accumulate the loss\n                test_loss += loss.item() * inputs.size(0)\n\n                # Update the number of samples\n                num_samples += inputs.size(0)\n\n                y_true.append(labels.cpu().numpy())  # Convert labels to numpy array and move to CPU\n                y_prob.append(prob.cpu().numpy())  # Convert probabilities to numpy array and move to CPU\n\n                predictions.extend(predicted.cpu().numpy())\n                true_labels.extend(labels.cpu().numpy())\n\n        y_true = np.concatenate(y_true)  # Concatenate true labels across all batches\n        y_prob = np.concatenate(y_prob)  # Concatenate predicted probabilities across all batches\n        # Compute the average loss\n        avg_test_loss = test_loss / num_samples\n        print(f\"Test Loss: {avg_test_loss:.4f}\")\n\n        self.metrics(np.array(true_labels), np.array(predictions), ['melanoma', 'nevus', 'seborrheic_keartosis'], y_true, y_prob)\n\n    def get_weights(self):\n        return self.model.state_dict()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ob1,ob2,ob3,ob4 = Client(1,'/kaggle/input/isic20174clients/kaggle/working/isic2017-clients/partition0'),Client(2,'/kaggle/input/isic20174clients/kaggle/working/isic2017-clients/partition1'),Client(3,'/kaggle/input/isic20174clients/kaggle/working/isic2017-clients/partition2'),Client(4,'/kaggle/input/isic20174clients/kaggle/working/isic2017-clients/partition3')\nnum_epochs=30","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ob1.train_model(epochs=num_epochs)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ob2.train_model(epochs=num_epochs,prev_models=[\"/kaggle/working/Client_1.pth\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ob3.train_model(epochs=num_epochs,prev_models=[\"/kaggle/working/Client_1.pth\",\"/kaggle/working/Client_2.pth\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ob4.train_model(epochs=num_epochs,prev_models=[\"/kaggle/working/Client_1.pth\",\"/kaggle/working/Client_2.pth\",\"/kaggle/working/Client_3.pth\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ob3.set_weights(\"/kaggle/working/Client_4.pth\")\nob2.set_weights(\"/kaggle/working/Client_4.pth\")\nob1.set_weights(\"/kaggle/working/Client_4.pth\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ob4.get_predictions()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ob3.get_predictions()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ob2.get_predictions()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ob1.get_predictions()","metadata":{},"execution_count":null,"outputs":[]}]}